{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Load COCO dataset\n",
    "dataset = fo.zoo.load_zoo_dataset(\"coco-2017\", split=\"validation\", max_samples=100)\n",
    "\n",
    "# Launch the app for interactive visualization\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d083bbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['boxes', 'labels', 'scores', 'masks'])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Use a raw string for windows path, and open the file in binary mode\n",
    "pkl_path = r\"C:\\Users\\john\\Documents\\programming_dirty\\me744_project\\datasets\\visualizations\\Fuji-Apple-Segmentation-Augmented\\pred_7.pkl\"\n",
    "\n",
    "with open(pkl_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Optionally, display the loaded data type or keys for inspection\n",
    "print(type(data))\n",
    "if isinstance(data, dict):\n",
    "    print(data.keys())\n",
    "\n",
    "print(data[\"boxes\"].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ac4f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john\\Documents\\programming_dirty\\me744_project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n",
      " 100% |█████████████████| 399/399 [12.9s elapsed, 0s remaining, 31.9 samples/s]      \n",
      "[{'id': 1, 'name': 'apple', 'supercategory': 'fruit'}, {'id': 2, 'name': 'branches', 'supercategory': 'tree'}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=cd8e0894-f471-4762-8572-8a1a6b096e31\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20df2e63200>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# data_path=\"Car-Parts-Segmentation/trainingset/JPEGImages\"\n",
    "# labels_path=\"Car-Parts-Segmentation/trainingset/annotations.json\"\n",
    "\n",
    "data_path = \"../datasets/Fuji-Apple-Segmentation_coco/trainingset/JPEGImages\"\n",
    "labels_path = \"../datasets/Fuji-Apple-Segmentation_coco/trainingset/annotations.json\"\n",
    "\n",
    "data_path = \"../datasets/image_envy_5000_coco/trainingset/JPEGImages\"\n",
    "labels_path = \"../datasets/image_envy_5000_coco/trainingset/annotations.json\"\n",
    "\n",
    "data_path = \"../datasets/Fuji-Apple-Segmentation_with_envy_mask_coco/trainingset/JPEGImages\"\n",
    "labels_path = \"../datasets/Fuji-Apple-Segmentation_with_envy_mask_coco/trainingset/annotations.json\"\n",
    "\n",
    "coco_dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    data_path=data_path,\n",
    "    labels_path=labels_path,\n",
    "    include_id=True,\n",
    ")\n",
    "\n",
    "# COCO categories are also imported\n",
    "print(coco_dataset.info[\"categories\"])\n",
    "# [{'id': 1, 'name': 'airplane', 'supercategory': None}, ...]\n",
    "\n",
    "# print(coco_dataset)\n",
    "session = fo.launch_app(coco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb28b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import CocoSegmentationDataset, build_model, detection_collate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "build_model(num_classes=9)\n",
    "\n",
    "# Play with COCO dataset\n",
    "train_ds = CocoSegmentationDataset(\n",
    "    \"Fuji-Apple-Segmentation/trainingset/JPEGImages\",\n",
    "    \"Fuji-Apple-Segmentation/trainingset/annotations.json\",\n",
    ")\n",
    "\n",
    "train_ds = CocoSegmentationDataset(\n",
    "    \"image_envy_5000_coco/trainingset/JPEGImages\",\n",
    "    \"image_envy_5000_coco/trainingset/annotations.json\",\n",
    ")  # train_ds = CocoSegmentationDataset(\n",
    "#     \"Car-Parts-Segmentation/trainingset/JPEGImages\",\n",
    "#     \"Car-Parts-Segmentation/trainingset/annotations.json\",\n",
    "# )\n",
    "# Print shape of all masks in the dataset\n",
    "# for idx in range(len(train_ds)):\n",
    "#     _, target = train_ds[idx]\n",
    "#     if \"masks\" in target:\n",
    "#         print(f\"Sample {idx}: mask shape = {target['masks'].shape}\")\n",
    "#     else:\n",
    "#         print(f\"Sample {idx}: No mask found\")\n",
    "\n",
    "\n",
    "def display_ds_item(train_ds, idx):\n",
    "    # __getitem__ usually returns (image, target)\n",
    "    img, target = train_ds[i]\n",
    "    img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    # Visualize mask (showing the merged mask from all instances)\n",
    "    if \"masks\" in target and target[\"masks\"].ndim == 3:\n",
    "        mask_all = target[\"masks\"].sum(dim=0)\n",
    "        axes[1].imshow(mask_all.cpu().numpy(), cmap=\"gray\")\n",
    "        axes[1].set_title(\"Combined Mask\")\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, \"No mask\", ha=\"center\", va=\"center\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "img_keys = list(train_ds.coco.imgs.keys())\n",
    "for idx in img_keys:\n",
    "    ann_ids = train_ds.coco.getAnnIds(imgIds=idx)\n",
    "    print(\"ann_ids:\", ann_ids)\n",
    "    print(f\"train_ds.coco.loadImgs({idx}):\")\n",
    "    print(train_ds.coco.loadImgs(idx))\n",
    "    anns = train_ds.coco.loadAnns(ann_ids)\n",
    "    # print(\"anns:\")\n",
    "    # print(anns)\n",
    "    for ann in anns:\n",
    "        print(\"train_ds.coco.annToMask(ann):\")\n",
    "        mask = train_ds.coco.annToMask(ann)\n",
    "        print(\"mask.shape:\", mask.shape)\n",
    "        print(\"mask.sum():\", mask.sum())\n",
    "\n",
    "# Visualize images and masks: show first 5 samples\n",
    "for i in range(15):\n",
    "    display_ds_item(train_ds, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4792eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = coco_dataset.first()\n",
    "print(sample)\n",
    "import json\n",
    "\n",
    "print(json.dumps(sample.to_dict(), indent=2))\n",
    "\n",
    "# Get the absolute path of the current file\n",
    "\n",
    "with open(\"filename\", \"w\") as f:\n",
    "    json.dump(sample.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e5bbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "files = [\n",
    "    x.name\n",
    "    for x in Path(\n",
    "        r\"C:\\Users\\john\\Documents\\programming_dirty\\me744_project\\datasets\\Fuji-Apple-Segmentation_with_envy_mask_coco\\trainingset\\JPEGImages\"\n",
    "    ).iterdir()\n",
    "    if x.is_file()\n",
    "]\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc48cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing ground truth for (1080, 1920, 3)\n",
      "Mask color: [216, 147, 236]\n",
      "Mask: [[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "Target: {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'masks': tensor([], size=(0, 1080, 1920), dtype=torch.uint8)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy.ndimage\n",
    "from PIL import Image\n",
    "\n",
    "APPLE_MASK_COLOR = [216, 147, 236]\n",
    "\n",
    "\n",
    "def construct_ground_truth(image_np, mask_color=APPLE_MASK_COLOR):\n",
    "    \"\"\"\n",
    "    Construct ground truth dict from an image and a specific color mask.\n",
    "    Args:\n",
    "        image_np: Label image as numpy array [H, W, C] (RGB or RGBA)\n",
    "        mask_color: List/Tuple of [R, G, B] to match\n",
    "    Returns:\n",
    "        target dict with boxes, labels, masks\n",
    "    \"\"\"\n",
    "    print(f\"Constructing ground truth for {image_np.shape}\")\n",
    "    print(f\"Mask color: {mask_color}\")\n",
    "    # Ensure image is numpy\n",
    "    img = np.array(image_np)\n",
    "\n",
    "    # Handle RGB vs RGBA: just check first 3 channels\n",
    "    if img.shape[-1] >= 3:\n",
    "        img_rgb = img[:, :, :3]\n",
    "    else:\n",
    "        img_rgb = img\n",
    "\n",
    "    # Create binary mask where color matches\n",
    "    # Using small tolerance for potential compression artifacts\n",
    "    # mask = np.all(img_rgb == mask_color, axis=-1)\n",
    "    mask = np.all(np.abs(img_rgb - mask_color) < 5, axis=-1)\n",
    "    print(f\"Mask: {mask}\")\n",
    "\n",
    "    if not np.any(mask):\n",
    "        return {\n",
    "            \"boxes\": torch.zeros((0, 4)),\n",
    "            \"labels\": torch.zeros((0,), dtype=torch.int64),\n",
    "            \"masks\": torch.zeros((0, img.shape[0], img.shape[1]), dtype=torch.uint8),\n",
    "        }\n",
    "\n",
    "    # Connected components to separate instances\n",
    "    labeled_mask, num_features = scipy.ndimage.label(mask)\n",
    "\n",
    "    boxes = []\n",
    "    masks = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(1, num_features + 1):\n",
    "        # boolean mask for this instance\n",
    "        instance_mask = labeled_mask == i\n",
    "\n",
    "        # Find bounding box\n",
    "        rows = np.any(instance_mask, axis=1)\n",
    "        cols = np.any(instance_mask, axis=0)\n",
    "        if not np.any(rows) or not np.any(cols):\n",
    "            continue\n",
    "\n",
    "        y1, y2 = np.where(rows)[0][[0, -1]]\n",
    "        x1, x2 = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "        # Box format: x1, y1, x2, y2\n",
    "        boxes.append([x1, y1, x2 + 1, y2 + 1])\n",
    "        masks.append(instance_mask)\n",
    "        labels.append(1)  # Default class 1\n",
    "\n",
    "    if not boxes:\n",
    "        return {\n",
    "            \"boxes\": torch.zeros((0, 4)),\n",
    "            \"labels\": torch.zeros((0,), dtype=torch.int64),\n",
    "            \"masks\": torch.zeros((0, img.shape[0], img.shape[1]), dtype=torch.uint8),\n",
    "        }\n",
    "\n",
    "    # Convert to tensors\n",
    "    target = {\n",
    "        \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "        \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "        \"masks\": torch.as_tensor(np.stack(masks), dtype=torch.uint8),  # [N, H, W]\n",
    "    }\n",
    "    print(f\"Target: {target}\")\n",
    "\n",
    "    return target\n",
    "\n",
    "img_path = \"C:/Users/john/Documents/programming_dirty/me744_project/maskrcnn_coco/test_img/4997_rgb_0001_label.png\"\n",
    "\n",
    "label_img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "target = construct_ground_truth(label_img)\n",
    "print(f\"Target: {target}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
